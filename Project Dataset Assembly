import pandas as pd
import requests, json, time, re
import spacy
from spacy.lang.en.stop_words import STOP_WORDS
import langdetect

API_URL = "https://api.themoviedb.org/3"
HEADERS = {
    "accept": "application/json",
    "Authorization": "Bearer eyJhbGciOiJIUzI1Ni9..."  # replace with your token
}

def get_movies(years):
    movies = []
    for year in years:
        page, total_pages = 1, 1
        while page <= min(total_pages, 200):
            url = f"{API_URL}/discover/movie?primary_release_year={year}&sort_by=vote_count.desc&page={page}"
            res = requests.get(url, headers=HEADERS)
            data = res.json()
            total_pages = data.get("total_pages", 1)
            for m in data.get("results", []):
                movies.append({"movie_id": m["id"], "title": m["original_title"], "year": year})
            page += 1
            time.sleep(0.25)
    print(f"Collected {len(movies)} movies across {len(years)} years.")
    return movies

def get_reviews(movie_id, title):
    reviews, page = [], 1
    while page <= 5:
        url = f"{API_URL}/movie/{movie_id}/reviews?page={page}"
        res = requests.get(url, headers=HEADERS)
        data = res.json()
        for r in data.get("results", []):
            reviews.append({
                "movie_id": movie_id,
                "title": title,
                "review": r["content"],
                "rating": r["author_details"].get("rating")
            })
        if page >= data.get("total_pages", 1):
            break
        page += 1
        time.sleep(0.25)
    return reviews

def assemble_raw_dataset(years):
    all_reviews = []
    movie_list = get_movies(years)
    for movie in movie_list:
        all_reviews.extend(get_reviews(movie["movie_id"], movie["title"]))
    df = pd.DataFrame(all_reviews)
    df = df.dropna(subset=["rating"])
    df = df[["title", "review", "rating"]]
    df.to_csv("tmdb_reviews_raw.csv", index=False)
    print("✅ Raw dataset saved as 'tmdb_reviews_raw.csv'")
    return df

nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])
NEGATIONS = ("not", "no", "never", "without", "none", "neither", "nor", "cannot")
CUSTOM_STOP = STOP_WORDS | set(NEGATIONS)
_contractions = [(r"n't"," not"),(r"'re"," are"),(r"'s",""),(r"'d"," would"),(r"'ll"," will"),(r"'ve"," have"),(r"'m"," am")]

def normalize_contractions(text):
    for pat, repl in _contractions:
        text = re.sub(pat, repl, text, flags=re.IGNORECASE)
    return text

def regex_clean(text):
    text = normalize_contractions(text)
    text = re.sub(r"<.*?>","",text)
    text = re.sub(r"http\S+","",text)
    text = re.sub(r"[^A-Za-z\s]","",text)
    return text.lower().strip()

def spacy_clean(text):
    doc = nlp(text)
    tokens = [t.lemma_.lower() for t in doc if t.is_alpha and t.lemma_.lower() not in CUSTOM_STOP and len(t.lemma_)>2]
    return " ".join(tokens)

def is_english(text):
    try:
        return langdetect.detect(text)=="en"
    except:
        return False

def is_ascii(text):
    return all(ord(c)<128 for c in text)

def clean_dataset(df):
    df["clean_text"]=df["review"].apply(regex_clean)
    df=df[df["clean_text"].apply(is_english)]
    df=df[df["title"].apply(is_ascii)]
    df["clean_text"]=df["clean_text"].apply(spacy_clean)
    df=df[df["clean_text"].str.len()>0]
    df.to_csv("cleaned_tmdb_reviews.csv",index=False)
    print("✅ Cleaned dataset saved as 'cleaned_tmdb_reviews.csv'")
    return df

if __name__=="__main__":
    YEARS=[2024,2023,2022,2021,2020]
    raw=assemble_raw_dataset(YEARS)
    clean_dataset(raw)
